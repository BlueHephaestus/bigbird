{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BigBirdForQuestionAnswering, BigBirdTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 846k/846k [00:01<00:00, 842kB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 775/775 [00:00<00:00, 200kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 943/943 [00:00<00:00, 212kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"vasudevgupta/bigbird-base-trivia-itc\"\n",
    "model = BigBirdForQuestionAnswering.from_pretrained(model_id, block_size=16, num_random_blocks=3)\n",
    "tokenizer = BigBirdTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch. Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context):\n",
    "    encoding = tokenizer(question, context, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True)\n",
    "    input_ids = encoding.input_ids\n",
    "    attention_mask = encoding.attention_mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_scores, end_scores = model(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
    "\n",
    "    # Let's take the most likely token using `argmax` and retrieve the answer\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
    "\n",
    "    answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
    "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'32'"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "question = \"How many pretrained models are available in ðŸ¤— Transformers?\"\n",
    "get_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}