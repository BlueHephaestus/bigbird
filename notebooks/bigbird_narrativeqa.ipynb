{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bigbird-narrativeqa.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vDpxD3vkcWnz",
        "oG77nBx74dyX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudevgupta7/bigbird-intuition/blob/main/notebooks/bigbird_narrativeqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBkRX_3fH8z3"
      },
      "source": [
        "# `BigBird`\n",
        "\n",
        "Let's explore how to use `BigBird` model with existing [`BertGeneration`](https://huggingface.co/transformers/model_doc/bertgeneration.html) and [`EncoderDecoderModel`](https://huggingface.co/transformers/model_doc/encoderdecoder.html).\n",
        "\n",
        "By the end of this tutorial, you will get an idea about:\n",
        "* How to use `BigBird` model for any task.\n",
        "* How ü§ó can handles your end2end integration (weights loading / saving, training, inference) in transformers.\n",
        "* How to use ü§ó datasets, Hub, & transformers (obviously!).\n",
        "* How awesome ü§ó is.\n",
        "\n",
        "**Note:** I am doing just an experiment with `BigBird` by putting BigBird in both Encoder and Decoder, hence not sure how well it's gonna performs. Let's see how it works üßê.\n",
        "\n",
        "Checkout my [LinkedIn](https://www.linkedin.com/in/vasudevgupta7/), [GitHub](https://github.com/vasudevgupta7), [Twitter](https://twitter.com/7vasudevgupta) if you wanna know what I do?\n",
        "\n",
        "Any kinda discussions regarding `BigBird` are *welcomed* through this [repo](https://github.com/vasudevgupta7/bigbird-intuition). Feel free to checkout my recent [post](https://github.com/vasudevgupta7/bigbird-intuition) on BigBird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDpxD3vkcWnz"
      },
      "source": [
        "## Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUNtR-JQ4rNt",
        "outputId": "9d866d91-6bb6-440a-8507-610e4d0cfeef"
      },
      "source": [
        "# do remember to link gdrive else you won't be able to save your weights\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQwq5vdO0Jtv",
        "outputId": "4b211c97-a688-4fa0-c25b-66c6d51881d2"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv1BuWnjxEXo"
      },
      "source": [
        "# BigBird tokenizer is relying on senetencepiece so we needa install it first\n",
        "\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/vasudevgupta7/transformers.git@add_big_bird\n",
        "!pip install sentencepiece\n",
        "!pip install wandb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG77nBx74dyX"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWeuJLqxxSkk"
      },
      "source": [
        "from datasets import load_dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AmFTnAu4W38"
      },
      "source": [
        "We will use [`narative-qa dataset`](https://huggingface.co/datasets/narrativeqa_manual) and finetune BigBird for abstractive question answering. This dataset requires manual download, we you will need to run next cell for that.\n",
        "\n",
        "It's gonna take some time (~10 mins) üôÅ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RriS8PXdzWS0",
        "outputId": "011801ee-692b-45f9-9745-47c87971638f"
      },
      "source": [
        "# this will download narrative-qa dataset into `narative-qa/tmp`\n",
        "!git clone https://github.com/deepmind/narrativeqa --branch master && sh narrativeqa/download_stories.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'narrativeqa' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfzEf4yx1fmm",
        "outputId": "0a9fbf8d-8cc5-4d4a-caa0-168070e445b1"
      },
      "source": [
        "# this may take upto 5 minutes\n",
        "\n",
        "dataset = load_dataset(\"narrativeqa_manual\", data_dir=\"narrativeqa/tmp\")\n",
        "dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-data_dir=narrativeqa%2Ftmp\n",
            "Reusing dataset narrativeqa_manual (/root/.cache/huggingface/datasets/narrativeqa_manual/default-data_dir=narrativeqa%2Ftmp/1.0.0/c57377ffa4fc72b25bf692f6676b140db5a36a7d36a56a891e11274afb40a6ba)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['document', 'question', 'answers'],\n",
              "        num_rows: 32747\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['document', 'question', 'answers'],\n",
              "        num_rows: 10557\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['document', 'question', 'answers'],\n",
              "        num_rows: 3461\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEHjP1c-x_aK",
        "outputId": "5e3e340f-381a-4599-cc70-64ac68507770"
      },
      "source": [
        "tr_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n",
        "tr_dataset, val_dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['document', 'question', 'answers'],\n",
              "     num_rows: 32747\n",
              " }), Dataset({\n",
              "     features: ['document', 'question', 'answers'],\n",
              "     num_rows: 3461\n",
              " }))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HEwqogD-guw"
      },
      "source": [
        "# data = dataset[\"train\"].map(lambda x: {\"seqlen\": (len(x[\"document\"][\"summary\"][\"text\"]) + len(x[\"question\"]))//4})\n",
        "# data = data.map(lambda x: {\"q_seqlen\": len(x[\"question\"][\"text\"])//4})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9naXi6DAzX4"
      },
      "source": [
        "# lets decide whether we should use BigBird `block_sparse` attention or `original_full` attention\n",
        "# min(data[\"seqlen\"]), sum(data[\"seqlen\"])/len(data[\"seqlen\"]), max(data[\"seqlen\"])\n",
        "\n",
        "# Since avg seqlen < 1024, we should use `original_full` but for the purpose of demonstarting `block_spare` lets try `block_sparse` attention only"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2bvhQnrEII5"
      },
      "source": [
        "# lets see the seqlen of question. This may give some idea about the value of `block_size`\n",
        "# min(data[\"q_seqlen\"]), sum(data[\"q_seqlen\"])/len(data[\"q_seqlen\"]), max(data[\"q_seqlen\"])\n",
        "\n",
        "# lets take `block_size=64`"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFJMUAoM4s9U"
      },
      "source": [
        "## Training BigBird"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQA59uHhV2hl"
      },
      "source": [
        "We will be using BigBird in both encoder & decoder (let's call it BigBird2BigBird may be). This is not a new idea, rather introduced in this [paper](https://arxiv.org/abs/1907.12461). One of the experiment involved in this paper put BERT in both encoder & decoder and trained this architecture by introducing randomly initialized `cross_attention_layer`.\n",
        "\n",
        "Let's start step by step:\n",
        "* Setup BigBird in encoder side. We will have block sparse attention here with `num_random_blocks=3` & `block_size=64`.\n",
        "* Setup BigBird in decoder side. Here will have normal attention (`original_full`) as it has to be autoregressive and there are very few target tokens to use `block_sparse`.\n",
        "* Connect encoder and decoder with ü§ó `EncoderDecoder` to be able to do abstractive question answering.\n",
        "* Setup tokenizer for converting text into numbers which our model can take. We will be using BigBird tokenizer simply."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46gBwIayFOMa",
        "outputId": "52de726d-3b48-4376-c300-fc7e73852962"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertGenerationEncoder, BertGenerationDecoder, BigBirdTokenizer\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import wandb\n",
        "\n",
        "\n",
        "model_id = \"google/bigbird-roberta-base\"\n",
        "\n",
        "encoder = BertGenerationEncoder.from_pretrained(model_id, block_size=64, num_random_blocks=3)\n",
        "decoder = BertGenerationDecoder.from_pretrained(model_id, add_cross_attention=True, is_decoder=True)\n",
        "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
        "\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_id)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BertGenerationEncoder: ['bert.pooler.weight', 'bert.pooler.bias', 'bert.embeddings.token_type_embeddings.weight']\n",
            "- This IS expected if you are initializing BertGenerationEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertGenerationEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BertGenerationDecoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.weight', 'bert.pooler.bias', 'bert.embeddings.token_type_embeddings.weight']\n",
            "- This IS expected if you are initializing BertGenerationDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertGenerationDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UbXcSrOJpop"
      },
      "source": [
        "SRC_MAXLEN = 832\n",
        "TGT_MAXLEN = 32"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4R0YSLaaHak"
      },
      "source": [
        "I like to setup `collate_fn` for tokenization. I feel it's more easy üòå. Next cell is doing simple tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5sxqzW2J9KS"
      },
      "source": [
        "def collate_fn(features):\n",
        "\n",
        "  context = [x[\"document\"][\"summary\"][\"text\"] for x in features]\n",
        "  question = [x[\"question\"][\"text\"] for x in features]\n",
        "  answer = [x[\"answers\"][0][\"text\"] for x in features]\n",
        "\n",
        "  # should not eliminate special tokens since question and context are should have `SEP` in middle\n",
        "  inputs = tokenizer(question, context, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=SRC_MAXLEN)\n",
        "  labels = tokenizer(answer, return_tensors=\"pt\", padding=True, truncation=True, max_length=TGT_MAXLEN)\n",
        "\n",
        "  return {\n",
        "      \"input_ids\": inputs.input_ids,\n",
        "      \"attention_mask\": inputs.attention_mask,\n",
        "      \"decoder_input_ids\": labels.input_ids,\n",
        "      \"labels\": labels.input_ids,\n",
        "      \"decoder_attention_mask\": labels.attention_mask,\n",
        "  }"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A3245A5Q0A5"
      },
      "source": [
        "You might be wondering why I am feeding same data to `labels` & `decoder_input_ids`. **Well!** this is because while calculating loss ü§ó `Trainer` is removing 1st token from `labels` & last token from `decoder_input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iZI6zI0e2PY"
      },
      "source": [
        "# wandb is just awesome. Let's set it up ..\n",
        "\n",
        "%env WANDB_PROJECT = 'BigBird-narrative-qa'\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_dLkfAd54s0"
      },
      "source": [
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"bigbird2bigbird-narrative-qa\",\n",
        "    overwrite_output_dir=False,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=4000,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=5,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=4000,\n",
        "    save_strategy=\"epoch\",\n",
        "    run_name=\"bigbird2bigbird-narrative-qa-experiment1\",\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"wandb\",\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True,\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Nzy_bKRi_j"
      },
      "source": [
        "It's very important to keep `remove_unused_columns=False` since otherwise ü§ó `Trainer` will delete all the colums because we are not tokenizing using `Dataset` rather inside `collate_function`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gInpnuj6ZIAk"
      },
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=tr_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoZ3DM76bScz"
      },
      "source": [
        "Time to train. Let's do it. Quite simple right! All thanks to ü§ó."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "ysjRT6UlagRi",
        "outputId": "f69aa207-54f4-47c3-fba7-a7abffec6177"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">bigbird2bigbird-narrative-qa-experiment1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/7vasudevgupta/%27BigBird-narrative-qa%27\" target=\"_blank\">https://wandb.ai/7vasudevgupta/%27BigBird-narrative-qa%27</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/7vasudevgupta/%27BigBird-narrative-qa%27/runs/2fs10fuo\" target=\"_blank\">https://wandb.ai/7vasudevgupta/%27BigBird-narrative-qa%27/runs/2fs10fuo</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/wandb/run-20210320_202808-2fs10fuo</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1936' max='20465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1936/20465 53:41 < 8:34:21, 0.60 it/s, Epoch 0.47/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.270100</td>\n",
              "      <td>5.820133</td>\n",
              "      <td>316.615500</td>\n",
              "      <td>10.931000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.962100</td>\n",
              "      <td>7.598300</td>\n",
              "      <td>316.152800</td>\n",
              "      <td>10.947000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.871400</td>\n",
              "      <td>9.087159</td>\n",
              "      <td>315.857600</td>\n",
              "      <td>10.957000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz5aWPOVagUQ"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djt2Jk8cLlg"
      },
      "source": [
        "## Inference Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTC4r6uEcKqo"
      },
      "source": [
        "def get_answer(question, context):\n",
        "    encoding = tokenizer(question, context, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True)\n",
        "    input_ids = encoding.input_ids\n",
        "    attention_mask = encoding.attention_mask\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_scores, end_scores = model(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
        "\n",
        "    # Let's take the most likely token using `argmax` and retrieve the answer\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
        "\n",
        "    answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
        "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
        "\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0kzYV8ncK3d"
      },
      "source": [
        "# model_id = \"vasudevgupta/bigbird2bigbird-narrative-qa\"\n",
        "\n",
        "# model = EncoderDecoderModel.from_pretrained(model_id, encoder_block_size=16, encoder_num_random_blocks=3)\n",
        "# tokenizer = BigBirdTokenizer.from_pretrained(model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1pB1heVdcSD"
      },
      "source": [
        "# context = \"ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch. Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset\"\n",
        "# question = \"What is ü§ó transformers?\"\n",
        "\n",
        "# get_answer(question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqRL4U9VbcvJ"
      },
      "source": [
        "We finally reached the end. Hoping you liked it. Well you are ready to use BigBird for all your tasks. This was the first tutorial on using ü§ó `BigBirdModel` for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llhCZPMvb6Kw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}